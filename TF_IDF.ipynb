{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Sua  tarefa  será  gerar  a  matriz  termo-documento  usando  TF-IDF  por  meio  da  aplicação  das \n",
        "fórmulas TF-IDF na matriz termo-documento criada com a utilização do algoritmo Bag of Words. Sobre \n",
        "o Corpus que recuperamos anteriormente. \n",
        " "
      ],
      "metadata": {
        "id": "OdZN3Pk4wXhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import X_OK\n",
        "#Aluno: Fernando Purkott\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "#nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "\n",
        "@Language.component(\"Space\")\n",
        "def set_custom_boundaries(doc):\n",
        "     for token in doc[:-1]:\n",
        "         if token.text.__contains__('  '):\n",
        "             doc[token.i+1].is_sent_start = True\n",
        "     return doc\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.add_pipe('Space', before=\"parser\")\n",
        "\n",
        "def excluiNovaLinha(value):\n",
        "    return ''.join(value.splitlines())\n",
        "\n",
        "\n",
        "\n",
        "blacklist = [\n",
        "    '[document]',\n",
        "    'noscript',\n",
        "    'header',\n",
        "    'html',\n",
        "    'meta',\n",
        "    'head', \n",
        "    'input',\n",
        "    'script',\n",
        "    'style'\n",
        "]\n",
        "\n",
        "def scrape(URL):\n",
        "  blacklist = [\n",
        "    '[document]',\n",
        "    'noscript',\n",
        "    'header',\n",
        "    'html',\n",
        "    'meta',\n",
        "    'head', \n",
        "    'input',\n",
        "    'script',\n",
        "    'style'\n",
        "  ]\n",
        "\n",
        "  req = requests.get(URL)\n",
        "  soup = BeautifulSoup(req.content)\n",
        "\n",
        "\n",
        "  txt = soup.find_all(text=True)\n",
        "  clean_txt = \"\"\n",
        "\n",
        "  for t in txt:\n",
        "\n",
        "    \n",
        "    if t.parent.name not in blacklist:\n",
        "      t = t.replace('—', ' ')\n",
        "      t = t.replace(',', ' ')\n",
        "      t = t.replace('.', ' ')\n",
        "      t = t.replace(\"'\", '')\n",
        "      t = t.replace(\"’\", ' ')\n",
        "      t = t.replace(\"‘\", ' ')\n",
        "      t = t.replace('-', ' ')\n",
        "      clean_txt += '{} '.format(t)\n",
        "\n",
        "  txt_doc = nlp(excluiNovaLinha(clean_txt))\n",
        "\n",
        "\n",
        "  sentences = list(txt_doc.sents)\n",
        "  return sentences\n",
        "\n",
        "def criaVocab(sentences):\n",
        "  vocab = []\n",
        "  sentenceWords = []\n",
        "  for s in sentences:\n",
        "    for i in s:\n",
        "      sentenceWords = str(i).split()\n",
        "      for j in sentenceWords:\n",
        "        j = j.lower()\n",
        "        if j not in stop_words and j.translate(str.maketrans('', '', string.punctuation)) != \"\":\n",
        "          vocab.append(j.translate(str.maketrans('', '', string.punctuation)))  \n",
        "      sentenceWords = []\n",
        "    vocab = unico(vocab)\n",
        "    return vocab\n",
        "  \n",
        "\n",
        "def unico(words):\n",
        "  seen = set()\n",
        "  return [x for x in words if not (x in seen or seen.add(x))]\n",
        "\n",
        "def vectorize(tokens, vocab):\n",
        "    vector=[]\n",
        "    for w in vocab:\n",
        "      x = 0\n",
        "      for i in tokens:\n",
        "        if i == w:\n",
        "          x +=1\n",
        "      vector.append(x)\n",
        "    return vector\n",
        "\n",
        "def BagOfWords(sentences):\n",
        "  vocab = criaVocab(sentences)\n",
        "  numSentences = 1\n",
        "  for i in sentences:\n",
        "    for j in i:\n",
        "      numSentences = numSentences + 1\n",
        "  bagofwords = [None] * numSentences\n",
        "  bagofwords[0] = vocab\n",
        "  currentSentence = 1\n",
        "  for i in sentences:\n",
        "    for j in i:\n",
        "      tokens = str(j).split()\n",
        "      for z in range(len(tokens)):\n",
        "        tokens[z] = tokens[z].lower()  \n",
        "      bagofwords[currentSentence] = vectorize(tokens, vocab)\n",
        "      currentSentence = currentSentence + 1\n",
        "  return bagofwords\n",
        "      \n",
        "def TF(sentences, bagofwords): \n",
        "  tf = np.zeros((len(bagofwords),len(bagofwords[0])))\n",
        "  currentSentence = 0\n",
        "  for i in sentences:\n",
        "    for j in i:\n",
        "      numberofwords = len(str(j).split())\n",
        "      for word in str(j).split():\n",
        "        if word in bagofwords[0]:\n",
        "          tf[currentSentence][bagofwords[0].index(word.lower())] = bagofwords[0].index(word.lower()) / numberofwords\n",
        "      currentSentence = 1\n",
        "  return tf\n",
        "\n",
        "def IDF(bagofwords):\n",
        "  errors = []\n",
        "  idf = np.zeros(len(bagofwords[0]))\n",
        "  currentSentence = 0\n",
        "  for w in bagofwords[0]:\n",
        "    x = 0      \n",
        "    for i in bagofwords[1:]:\n",
        "      if i[bagofwords[0].index(w)] > 0: x += 1\n",
        "    try:     \n",
        "      idf[bagofwords[0].index(w)] =  np.log10(len(bagofwords[1:]) / x)\n",
        "    except:\n",
        "       idf[bagofwords[0].index(w)] = 1\n",
        "  return idf\n",
        "\n",
        "def TFIDF(tf,idf):\n",
        "  tfidf = tf.copy()\n",
        "  for i in range(len(tfidf)):\n",
        "    for j in range(len(tfidf[0])):\n",
        "      tfidf[i][j] = tfidf[i][j] * idf[j]\n",
        "  return tfidf\n",
        "sentences1 = scrape(\"https://www.ibm.com/cloud/learn/natural-language-processing\")\n",
        "sentences2 = scrape(\"https://hbr.org/2022/04/the-power-of-natural-language-processing\")\n",
        "sentences3 = scrape(\"https://link.springer.com/article/10.1007/s11042-022-13428-4\")\n",
        "sentences4 = scrape(\"https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP\")\n",
        "sentences5 = scrape(\"https://monkeylearn.com/natural-language-processing/\")\n",
        "\n",
        "sentences = [sentences1, sentences2, sentences3, sentences4, sentences5]\n",
        "bagofwords = BagOfWords(sentences)\n",
        "tf = TF(sentences, bagofwords)\n",
        "idf = IDF(bagofwords)\n",
        "tfidf = TFIDF(tf,idf)\n",
        "for i in tfidf:\n",
        "  print(i)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "nIbYFUhUMiG8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}